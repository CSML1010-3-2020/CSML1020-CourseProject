{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CSML1020 Course Project - New Plant Diseases Dectection\n## Authors (Group 3): Paul Doucet, Jerry Khidaroo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adjust pandas display\npd.options.display.max_columns = 30\npd.options.display.max_rows = 100\npd.options.display.float_format = '{:.2f}'.format\npd.options.display.precision = 2\npd.options.display.max_colwidth = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import matplotlib and seaborn and adjust some defaults\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.dpi'] = 100\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of all directories and file counts in given path\ndef getDirCounts(dirName):\n    listOfFile = os.listdir(dirName)\n    rows = []\n    # Iterate over all the entries\n    for entry in listOfFile:\n        # Create full path\n        fullPath = os.path.join(dirName, entry)\n        if os.path.isdir(fullPath):\n            fields = entry.split(\"___\")\n            if fields[1] == 'healthy':\n                status = 'healthy'\n            else:\n                status = \"unhealthy\"\n            \n            disease = fields[1].split(\"_\")[-1].replace(\")\",\"\").lower()\n            \n            noOfFiles = sum([len(files) for r, d, files in os.walk(fullPath)])\n            rows.append([fields[0], fields[1], noOfFiles, status, disease])\n\n    df = pd.DataFrame(rows, columns=[\"plant\", \"condition\", \"count\", \"status\", \"disease\"])\n    #df['plant_status'] = df['plant'] + ' - ' + df['status']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataDirTrain = '../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train'\ndataDirValidate = '../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/valid'\ndataDirTest = '../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the list of all files in directory tree at given path\nplants_df = getDirCounts(dataDirTrain)\nplants_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"physical_devices = tf.config.experimental.list_physical_devices('GPU')\nfor physical_device in physical_devices:\n    tf.config.experimental.set_memory_growth(physical_device, True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-Processing"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\n# batch_size = 128\nbatch_size = 48\ntraining_set = train_datagen.flow_from_directory(dataDirTrain,\n                                                 target_size=(224, 224),\n                                                 batch_size=batch_size,\n                                                 class_mode='categorical')\n\nvalid_set = valid_datagen.flow_from_directory(dataDirValidate,\n                                            target_size=(224, 224),\n                                            batch_size=batch_size,\n                                            class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"class_dict = training_set.class_indices\nprint(class_dict)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"li = list(class_dict.keys())\nprint(li)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_num = training_set.samples\nvalid_num = valid_set.samples","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model=ResNet50(include_top=False,input_shape=(224,224,3))\n    base_model.trainable=False\n\n    classifier=keras.models.Sequential()\n    classifier.add(base_model)\n    classifier.add(Flatten())\n    classifier.add(Dense(38,activation='softmax'))\n    \n    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Save_Benchmark(descr, metric, reset_rb, init_rb):\n    global rows_benchmarks\n    #global rows_benchmarks_all\n    global df_benchmarks\n    #global df_benchmarks_all\n    # if (rows_benchmarks == None):\n    #     rows_benchmarks = []\n    if (init_rb):\n        rows_benchmarks = []\n    else:\n        if (reset_rb):\n            rows_benchmarks = []\n\n        # if (reset_rb_all):\n        #     rows_benchmarks_all = []\n        rows_benchmarks.append([descr, metric])\n        #rows_benchmarks_all.append([descr, metric])\n        df_benchmarks = pd.DataFrame(rows_benchmarks, columns=[\"Preprosessing Steps\", \"accuracy\"])\n        #df_benchmarks_all = pd.DataFrame(rows_benchmarks_all, columns=[\"Preprosessing Steps\", \"accuracy\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model(train_datagen, valid_set, batch_size, epochs):    \n    training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n\n    train_num = training_set.samples\n    valid_num = valid_set.samples\n\n    classifier = get_model()\n    \n    #fitting images to CNN\n    history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size)\n\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Model Preparation VGG16"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=VGG16(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=keras.models.Sequential()\nclassifier.add(base_model)\nclassifier.add(Flatten())\nclassifier.add(Dense(38,activation='softmax'))\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_img_file = '/tmp/model_VGG16.png'\ntf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom packaging import version\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(\"TensorFlow version: \", tf.__version__)\nassert version.parse(tf.__version__).release[0] >= 2, \\\n    \"This notebook requires TensorFlow 2.0 or above.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier.compile(optimizer='adam',\n#              loss='categorical_crossentropy',\n#              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting images to CNN\n#history = classifier.fit(training_set,\n#                         steps_per_epoch=train_num//batch_size,\n#                         validation_data=valid_set,\n#                         epochs=3,\n#                         validation_steps=valid_num//batch_size,\n#                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving our model\n#filepath=\"./Mymodel_VGG16.h5\"\n#classifier.save(filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model=VGG16(include_top=False,input_shape=(224,224,3))\n    base_model.trainable=False\n\n    classifier=keras.models.Sequential()\n    classifier.add(base_model)\n    classifier.add(Flatten())\n    classifier.add(Dense(38,activation='softmax'))\n    \n    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Save_Benchmark(descr, metric, reset_rb, init_rb):\n    global rows_benchmarks\n    #global rows_benchmarks_all\n    global df_benchmarks\n    #global df_benchmarks_all\n    # if (rows_benchmarks == None):\n    #     rows_benchmarks = []\n    if (init_rb):\n        rows_benchmarks = []\n    else:\n        if (reset_rb):\n            rows_benchmarks = []\n\n        # if (reset_rb_all):\n        #     rows_benchmarks_all = []\n        rows_benchmarks.append([descr, metric])\n        #rows_benchmarks_all.append([descr, metric])\n        df_benchmarks = pd.DataFrame(rows_benchmarks, columns=[\"Preprosessing Steps\", \"accuracy\"])\n        #df_benchmarks_all = pd.DataFrame(rows_benchmarks_all, columns=[\"Preprosessing Steps\", \"accuracy\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model(train_datagen, valid_set, batch_size, epochs):    \n    training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n\n    train_num = training_set.samples\n    valid_num = valid_set.samples\n\n    classifier = get_model()\n    \n    #fitting images to CNN\n    history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size)\n\n    return history","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"dg_descrs = [\"rescale=1./255\", \n    #\"rescale=1./255, shear_range=0.2\", \n    #\"rescale=1./255, zoom_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\",\n    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2\",\n    #\"rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\"\n]\n\ndatagens = [\n    ImageDataGenerator(rescale=1./255, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, width_shift_range=0.2, fill_mode='nearest'),\n    #mageDataGenerator(rescale=1./255, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0], fill_mode='nearest'),\n]\n\n\nbatch_size = 48\nepochs = 10\nvalid_datagen = ImageDataGenerator(rescale=1./255)\nvalid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\nSave_Benchmark(\"\", \"\", False, True)\n\nfor train_datagen, dg_descr in zip(datagens, dg_descrs):\n    history = run_model(train_datagen, valid_set, batch_size, epochs)\n    #history.history\n    Save_Benchmark(dg_descr, history.history['val_accuracy'][2], False, False)\n\ndf_benchmarks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference - VGG16"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test/CornCommonRust1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = classifier.predict(valid_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Model Preparation ResNet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of loading the resnet50 model\nfrom keras.applications.resnet50 import ResNet50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=ResNet50(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"classifier=keras.models.Sequential()\nclassifier.add(base_model)\nclassifier.add(Flatten())\nclassifier.add(Dense(38,activation='softmax'))\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_img_file = '/tmp/model_resnet50.png'\ntf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier.compile(optimizer='adam',\n#              loss='categorical_crossentropy',\n#              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#fitting images to CNN\n#history = classifier.fit(training_set,\n#                         steps_per_epoch=train_num//batch_size,\n#                         validation_data=valid_set,\n#                         epochs=3,\n#                         validation_steps=valid_num//batch_size,\n#                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving our model\n#filepath=\"./Mymodel_sample_resnet50.h5\"\n#classifier.save(filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model=ResNet50(include_top=False,input_shape=(224,224,3))\n    base_model.trainable=False\n\n    classifier=keras.models.Sequential()\n    classifier.add(base_model)\n    classifier.add(Flatten())\n    classifier.add(Dense(38,activation='softmax'))\n    \n    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Save_Benchmark(descr, metric, reset_rb, init_rb):\n    global rows_benchmarks\n    #global rows_benchmarks_all\n    global df_benchmarks\n    #global df_benchmarks_all\n    # if (rows_benchmarks == None):\n    #     rows_benchmarks = []\n    if (init_rb):\n        rows_benchmarks = []\n    else:\n        if (reset_rb):\n            rows_benchmarks = []\n\n        # if (reset_rb_all):\n        #     rows_benchmarks_all = []\n        rows_benchmarks.append([descr, metric])\n        #rows_benchmarks_all.append([descr, metric])\n        df_benchmarks = pd.DataFrame(rows_benchmarks, columns=[\"Preprosessing Steps\", \"accuracy\"])\n        #df_benchmarks_all = pd.DataFrame(rows_benchmarks_all, columns=[\"Preprosessing Steps\", \"accuracy\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model(train_datagen, valid_set, batch_size, epochs):    \n    training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n\n    train_num = training_set.samples\n    valid_num = valid_set.samples\n\n    classifier = get_model()\n    \n    #fitting images to CNN\n    history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size)\n\n    return history","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"dg_descrs = [\"rescale=1./255\", \n    \"rescale=1./255, shear_range=0.2\", \n    #\"rescale=1./255, zoom_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\",\n    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2\",\n    #\"rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\"\n]\n\ndatagens = [\n    ImageDataGenerator(rescale=1./255, fill_mode='nearest'),\n    ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, width_shift_range=0.2, fill_mode='nearest'),\n    #mageDataGenerator(rescale=1./255, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0], fill_mode='nearest'),\n]\n\n\nbatch_size = 48\nepochs = 5\nvalid_datagen = ImageDataGenerator(rescale=1./255)\nvalid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\nSave_Benchmark(\"\", \"\", False, True)\n\nfor train_datagen, dg_descr in zip(datagens, dg_descrs):\n    history = run_model(train_datagen, valid_set, batch_size, epochs)\n    #history.history\n    Save_Benchmark(dg_descr, history.history['val_accuracy'][2], False, False)\n\ndf_benchmarks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference - ResNET50"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test/CornCommonRust1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_resnet_50 = classifier.predict(valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Model Preparation InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of loading the InceptionV3 model\nfrom keras.applications import InceptionV3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=InceptionV3(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=InceptionV3(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False\n\nclassifier=keras.models.Sequential()\nclassifier.add(base_model)\nclassifier.add(Flatten())\nclassifier.add(Dense(38,activation='softmax'))\n    \nclassifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \ndot_img_file = '/tmp/model_InceptionV3.png'\ntf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model=InceptionV3(include_top=False,input_shape=(224,224,3))\n    base_model.trainable=False\n\n    classifier=keras.models.Sequential()\n    classifier.add(base_model)\n    classifier.add(Flatten())\n    classifier.add(Dense(38,activation='softmax'))\n    \n    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    dot_img_file = '/tmp/model_InceptionV3.png'\n    tf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)\n    \n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dg_descrs = [\"rescale=1./255\", \n    #\"rescale=1./255, shear_range=0.2\", \n    #\"rescale=1./255, zoom_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\",\n    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2\",\n    #\"rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\"\n]\n\ndatagens = [\n    ImageDataGenerator(rescale=1./255, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, width_shift_range=0.2, fill_mode='nearest'),\n    #mageDataGenerator(rescale=1./255, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0], fill_mode='nearest'),\n]\n\n\nbatch_size = 48\nepochs = 10\nvalid_datagen = ImageDataGenerator(rescale=1./255)\nvalid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\nSave_Benchmark(\"\", \"\", False, True)\n\nfor train_datagen, dg_descr in zip(datagens, dg_descrs):\n    history = run_model(train_datagen, valid_set, batch_size, epochs)\n    #history.history\n    Save_Benchmark(dg_descr, history.history['val_accuracy'][2], False, False)\n\ndf_benchmarks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference - InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test/CornCommonRust1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_inceptV3 = classifier.predict(valid_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Review of a Model File"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python program to demonstrate \n# HDF5 file \n  \nimport numpy as np \nimport h5py \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing a random numpy array \narr = np.random.randn(1000) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a file \nwith h5py.File('test.hdf5', 'w') as f:  \n    dset = f.create_dataset(\"default\", data = arr) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open the file as 'f' \nwith h5py.File('../input/trialh5view/Mymodel_VGG16.h5', 'r') as f:  \n    data = f['default'] \n      \n    # get the minimum value \n    print(min(data))  \n      \n    # get the maximum value \n    print(max(data)) \n      \n    # get the values ranging from index 0 to 15 \n    print(data[:15]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## View Weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of loading the InceptionV3 model\nfrom keras.applications import InceptionV3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=InceptionV3(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"## Xception"},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of loading the Xception model\nfrom keras.applications import Xception","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=Xception(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model=Xception(include_top=False,input_shape=(224,224,3))\nbase_model.trainable=False\n\nclassifier=keras.models.Sequential()\nclassifier.add(base_model)\nclassifier.add(Flatten())\nclassifier.add(Dense(38,activation='softmax'))\n    \nclassifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \ndot_img_file = '/tmp/model_InceptionV3.png'\ntf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model=Xception(include_top=False,input_shape=(224,224,3))\n    base_model.trainable=False\n\n    classifier=keras.models.Sequential()\n    classifier.add(base_model)\n    classifier.add(Flatten())\n    classifier.add(Dense(38,activation='softmax'))\n    \n    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    dot_img_file = '/tmp/model_InceptionV3.png'\n    tf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)\n    \n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dg_descrs = [\"rescale=1./255\", \n    #\"rescale=1./255, shear_range=0.2\", \n    #\"rescale=1./255, zoom_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\", \n    #\"rescale=1./255, width_shift_range=0.2\",\n    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2\",\n    #\"rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\"\n]\n\ndatagens = [\n    ImageDataGenerator(rescale=1./255, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, width_shift_range=0.2, fill_mode='nearest'),\n    #mageDataGenerator(rescale=1./255, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest'),\n    #ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0], fill_mode='nearest'),\n]\n\n\nbatch_size = 48\nepochs = 10\nvalid_datagen = ImageDataGenerator(rescale=1./255)\nvalid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\nSave_Benchmark(\"\", \"\", False, True)\n\nfor train_datagen, dg_descr in zip(datagens, dg_descrs):\n    history = run_model(train_datagen, valid_set, batch_size, epochs)\n    #history.history\n    Save_Benchmark(dg_descr, history.history['val_accuracy'][2], False, False)\n\ndf_benchmarks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import seaborn as sns\nsns.set()\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\n#accuracy plot\nplt.plot(epochs, acc, color='green', label='Training Accuracy')\nplt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.figure()\n#loss plot\nplt.plot(epochs, loss, color='pink', label='Training Loss')\nplt.plot(epochs, val_loss, color='red', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)## Inference - InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting an image\nfrom keras.preprocessing import image\nimport numpy as np\nimage_path = \"../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test/CornCommonRust1.JPG\"\nnew_img = image.load_img(image_path, target_size=(224, 224))\nimg = image.img_to_array(new_img)\nimg = np.expand_dims(img, axis=0)\nimg = img/255\n\nprint(\"Following is our prediction:\")\nprediction = classifier.predict(img)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nd = prediction.flatten()\nj = d.max()\nfor index,item in enumerate(d):\n    if item == j:\n        class_name = li[index]\n\n#ploting image with predicted class name        \nplt.figure(figsize = (4,4))\nplt.imshow(new_img)\nplt.axis('off')\nplt.title(class_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}