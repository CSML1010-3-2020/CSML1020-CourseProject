{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSML1020 Course Project - New Plant Diseases Dectection\n",
    "#### Authors (Group 3): Paul Doucet, Jerry Khidaroo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust pandas display\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn and adjust some defaults\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to enable GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use this to disable GPU\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Benchmarking Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "def get_alexnet_model():\n",
    "    # Initializing the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Convolution Step 1\n",
    "    classifier.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 1\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 2\n",
    "    classifier.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 2\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 3\n",
    "    classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 4\n",
    "    classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 5\n",
    "    classifier.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 3\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Flattening Step\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Full Connection Step\n",
    "    classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.4))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.4))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 1000, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 38, activation = 'softmax'))\n",
    "\n",
    "    classifier.load_weights('best_weights_9.hdf5')\n",
    "\n",
    "    # we chose to train the top 2 conv blocks, i.e. we will freeze\n",
    "    # the first 8 layers and unfreeze the rest:\n",
    "    for i, layer in enumerate(classifier.layers[:20]):\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compiling the Model\n",
    "    classifier.compile(optimizer=optimizers.SGD(lr=0.001, momentum=0.9, decay=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "   \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(base_model):\n",
    "    #base_model=VGG16(include_top=False,input_shape=(224,224,3))\n",
    "    base_model.trainable=False\n",
    "\n",
    "    classifier=keras.models.Sequential()\n",
    "    classifier.add(base_model)\n",
    "    classifier.add(Flatten())\n",
    "    \n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_A():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_B():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_C():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_D():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(38,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "classifier.add(MaxPooling2D((2, 2)))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "classifier.add(MaxPooling2D((2, 2)))\n",
    "classifier.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "classifier.add(MaxPooling2D((2, 2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "classifier.add(Dense(38,activation='softmax'))\n",
    "\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_img_file = '/tmp/model_D.png'\n",
    "tf.keras.utils.plot_model(classifier, to_file=dot_img_file, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_01():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n",
    "    #classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n",
    "    #classifier.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n",
    "    classifier.add(MaxPooling2D((2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    #classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    #classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation='softmax'))\n",
    "    \n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Benchmark(descr, metric, reset_rb, init_rb):\n",
    "    global rows_benchmarks\n",
    "    global df_benchmarks\n",
    "    if (init_rb):\n",
    "        rows_benchmarks = []\n",
    "    else:\n",
    "        if (reset_rb):\n",
    "            rows_benchmarks = []\n",
    "\n",
    "        rows_benchmarks.append([descr, metric])\n",
    "        df_benchmarks = pd.DataFrame(rows_benchmarks, columns=[\"Pre-Prosessing Steps\", \"Validation Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Model_Benchmark(descr, history, reset_rb, init_rb):\n",
    "    global rows_model_benchmarks\n",
    "    global df_model_benchmarks\n",
    "    if (init_rb):\n",
    "        rows_model_benchmarks = []\n",
    "    else:\n",
    "        if (reset_rb):\n",
    "            rows_model_benchmarks = []\n",
    "\n",
    "        rows_model_benchmarks.append([descr, history.history['accuracy'][-1], history.history['loss'][-1], history.history['val_accuracy'][-1], \n",
    "            history.history['val_loss'][-1],])\n",
    "        df_model_benchmarks = pd.DataFrame(rows_model_benchmarks, columns=[\"Classifier\", \"Accuracy\", \"Loss\", \"Validation Accuracy\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_datagen, valid_set, batch_size, epochs):    \n",
    "    training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "    train_num = training_set.samples\n",
    "    valid_num = valid_set.samples\n",
    "\n",
    "    base_model=VGG16(include_top=False,input_shape=(224,224,3))\n",
    "    classifier = get_model(base_model)\n",
    "    \n",
    "    #fitting images to CNN\n",
    "    history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "def plot_history(model, history, axs, row):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    #accuracy plot   \n",
    "    axs[row, 0].plot(epochs, acc, color='green', label='Training Accuracy')\n",
    "    axs[row, 0].plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n",
    "    axs[row, 0].set_title(model + ' Training and Validation Accuracy')\n",
    "    axs[row, 0].set_xlabel('Epoch')\n",
    "    axs[row, 0].set_ylabel('Accuracy')\n",
    "    axs[row, 0].legend()\n",
    "    \n",
    "    #loss plot   \n",
    "    axs[row, 1].plot(epochs, loss, color='pink', label='Training Loss')\n",
    "    axs[row, 1].plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "    axs[row, 1].set_title(model + ' Training and Validation Loss')\n",
    "    axs[row, 1].set_xlabel('Epoch')\n",
    "    axs[row, 1].set_ylabel('Accuracy')\n",
    "    axs[row, 1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Folders\n",
    "# dataDirTrain = '../NewPlantDiseasesDatasetSample/train'\n",
    "# dataDirValidate = '../NewPlantDiseasesDatasetSample/valid'\n",
    "# dataDirTest = '../NewPlantDiseasesDatasetSample/test'\n",
    "\n",
    "#dataDirTrain = '../NewPlantDiseasesDataset/train'\n",
    "#dataDirValidate = '../NewPlantDiseasesDataset/valid'\n",
    "#dataDirTest = '../NewPlantDiseasesDataset/test'\n",
    "\n",
    "# From Kaggle Site\n",
    "dataDirTrain = '../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
    "dataDirValidate = '../input/new-plant-diseases-dataset/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/valid'\n",
    "dataDirTest = '../input/newplantdiseasessample/NewPlantDiseasesDatasetSample/test'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dg_descrs = [\"rescale=1./255\", \n",
    "    \"rescale=1./255, shear_range=0.2\", \n",
    "    #\"rescale=1./255, zoom_range=0.2\", \n",
    "    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2\",\n",
    "    #\"rescale=1./255, shear_range=0.2, zoom_range=0.2, brightness_range=[0.2,1.0]\",\n",
    "    # \"rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2\",\n",
    "    # \"rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0]\"\n",
    "]\n",
    "\n",
    "datagens = [\n",
    "    ImageDataGenerator(rescale=1./255, fill_mode='nearest'),\n",
    "    ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest'),\n",
    "    #ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest'),\n",
    "    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, fill_mode='nearest'),\n",
    "    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, brightness_range=[0.2,1.0], fill_mode='nearest'),\n",
    "    #ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest'),\n",
    "    #ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=90, brightness_range=[0.2,1.0], zoom_range=[0.5,1.0], fill_mode='nearest'),\n",
    "]\n",
    "\n",
    "batch_size = 48\n",
    "epochs = 3\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "Save_Benchmark(\"\", \"\", False, True)\n",
    "\n",
    "for train_datagen, dg_descr in zip(datagens, dg_descrs):\n",
    "    history = run_model(train_datagen, valid_set, batch_size, epochs)\n",
    "    Save_Benchmark(dg_descr, history.history['val_accuracy'][2], False, False)\n",
    "    \n",
    "df_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Train Image Data with Best Augmentation Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest')\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# batch_size = 128\n",
    "batch_size = 32\n",
    "training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_dict = training_set.class_indices\n",
    "# print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# li = list(class_dict.keys())\n",
    "# print(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Models\n",
    "Base_model_ids = ['Model_A', 'Model_B', 'Model_C', 'Model_D']\n",
    "Base_models = [\n",
    "    get_model_A(),\n",
    "    get_model_B(),\n",
    "    get_model_C(),\n",
    "    get_model_D(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# checkpoint for Alexnet Model\n",
    "#weightpath = \"best_weights_9.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint]# checkpoint\n",
    "epochs = 3\n",
    "\n",
    "# Initialize function\n",
    "Save_Model_Benchmark(\"\", \"\", False, True)\n",
    "\n",
    "# Train Models and Save Histories\n",
    "histories = []\n",
    "for classifier, model_id in zip(Base_models, Base_model_ids):\n",
    "    history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size, )\n",
    "    Save_Model_Benchmark(model_id, history, False, False)\n",
    "    histories.append(history)\n",
    "\n",
    "df_model_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "# Plot Accuacy and Loss for Model Histories\n",
    "fig, axs = plt.subplots(5, 2, figsize=(12,18))\n",
    "\n",
    "row = 0\n",
    "for history, model_id in zip(histories, Base_model_ids): \n",
    "    plot_history(model_id, history, axs, row)\n",
    "    row = row + 1\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.3, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "# Print Model Layers\n",
    "for classifier, model_id in zip(Base_models, Base_model_ids):\n",
    "    print('Visualization of Layers for: ', model_id, ' base Model with Transfer Learning')\n",
    "    print('')\n",
    "    display(SVG(model_to_dot(classifier, show_shapes=True, show_layer_names=False, dpi=70, rankdir='TB').create(prog='dot', format='svg')))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Models to Investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# Initialize Models\n",
    "model_ids = ['VGG16', 'ResNet50', 'InceptionV3', 'Alexnet', 'Model_01']\n",
    "models = [\n",
    "    get_model(VGG16(include_top=False,input_shape=(224,224,3))),\n",
    "    get_model(ResNet50(include_top=False,input_shape=(224,224,3))),\n",
    "    get_model(InceptionV3(include_top=False,input_shape=(224,224,3))),\n",
    "    get_alexnet_model(),\n",
    "    get_model_01(),\n",
    "]\n",
    "\n",
    "# model_ids = ['Model_01']\n",
    "# models = [\n",
    "#     get_model_01(),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# # Print Model Layers\n",
    "# for classifier, model_id in zip(models, model_ids):\n",
    "#     print('Visualization of Layers for: ', model_id, ' base Model with Transfer Learning')\n",
    "#     print('')\n",
    "#     display(SVG(model_to_dot(classifier, show_shapes=True, show_layer_names=False, dpi=70, rankdir='TB').create(prog='dot', format='svg')))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# checkpoint for Alexnet Model\n",
    "weightpath = \"best_weights_9.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]# checkpoint\n",
    "epochs = 3\n",
    "\n",
    "# Initialize function\n",
    "Save_Model_Benchmark(\"\", \"\", False, True)\n",
    "\n",
    "# Train Models and Save Histories\n",
    "histories = []\n",
    "for classifier, model_id in zip(models, model_ids):\n",
    "    if model_id == 'Alexnet':\n",
    "        history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size, callbacks=callbacks_list, )  \n",
    "    else:\n",
    "          history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size, )\n",
    "    Save_Model_Benchmark(model_id, history, False, False)\n",
    "    histories.append(history)\n",
    "\n",
    "df_model_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "# Plot Accuacy and Loss for Model Histories\n",
    "fig, axs = plt.subplots(5, 2, figsize=(12,18))\n",
    "\n",
    "row = 0\n",
    "for history, model_id in zip(histories, model_ids): \n",
    "    plot_history(model_id, history, axs, row)\n",
    "    row = row + 1\n",
    "\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.3, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fitting images to CNN\n",
    "# history = classifier.fit(training_set,\n",
    "#                          steps_per_epoch=train_num//batch_size,\n",
    "#                          validation_data=valid_set,\n",
    "#                          epochs=3,\n",
    "#                          validation_steps=valid_num//batch_size,\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Hyper-Parameter Tuning on Selected Model (Done in Plant_Disease_Detection_Augm_hp_tuning.ipynb due to errors in GPU mode.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Run Selected Model with Tuned Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "activation = 'softmax'\n",
    "epochs = 3\n",
    "\n",
    "classifier = create_model(activation, learning_rate)\n",
    "\n",
    "history = classifier.fit(training_set, steps_per_epoch=train_num//batch_size, validation_data=valid_set, epochs=epochs, validation_steps=valid_num//batch_size, )    \n",
    "Save_Model_Benchmark(\"VGG16 HP Tuned\", history, False, False)\n",
    "histories.append(history)\n",
    "\n",
    "df_model_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model\n",
    "filepath=\"./Mymodel_sample.h5\"\n",
    "classifier.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicting an image\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "image_path = \"../NewPlantDiseasesDataset/test/Tomato_Late_blight01.JPG\"\n",
    "new_img = image.load_img(image_path, target_size=(224, 224))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img/255\n",
    "\n",
    "print(\"Following is our prediction:\")\n",
    "prediction = classifier.predict(img)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "d = prediction.flatten()\n",
    "j = d.max()\n",
    "for index,item in enumerate(d):\n",
    "    if item == j:\n",
    "        class_name = li[index]\n",
    "\n",
    "#ploting image with predicted class name        \n",
    "plt.figure(figsize = (4,4))\n",
    "plt.imshow(new_img)\n",
    "plt.axis('off')\n",
    "plt.title(class_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
