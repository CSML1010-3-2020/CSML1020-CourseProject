{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSML1020 Course Project - New Plant Diseases Dectection\n",
    "#### Authors (Group 3): Paul Doucet, Jerry Khidaroo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to disable GPU\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Folders\n",
    "# dataDirTrain = '../NewPlantDiseasesDatasetSample/train'\n",
    "# dataDirValidate = '../NewPlantDiseasesDatasetSample/valid'\n",
    "# dataDirTest = '../NewPlantDiseasesDatasetSample/test'\n",
    "\n",
    "dataDirTrain = '../NewPlantDiseasesDataset/train'\n",
    "dataDirValidate = '../NewPlantDiseasesDataset/valid'\n",
    "dataDirTest = '../NewPlantDiseasesDataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Train Image Data with Best Augmentation Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 70295 images belonging to 38 classes.\nFound 17572 images belonging to 38 classes.\n"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, fill_mode='nearest')\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# batch_size = 128\n",
    "batch_size = 48\n",
    "training_set = train_datagen.flow_from_directory(dataDirTrain, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "valid_set = valid_datagen.flow_from_directory(dataDirValidate, target_size=(224, 224), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Hyper-Parameter Tuning on Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "def create_model_alexnet(activation='softmax', learning_rate=0.01):\n",
    "    # Initializing the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Convolution Step 1\n",
    "    classifier.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 1\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 2\n",
    "    classifier.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 2\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 3\n",
    "    classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 4\n",
    "    classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Convolution Step 5\n",
    "    classifier.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n",
    "\n",
    "    # Max Pooling Step 3\n",
    "    classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    # Flattening Step\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Full Connection Step\n",
    "    classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.4))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.4))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 1000, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dense(units = 38, activation = activation))\n",
    "\n",
    "    classifier.load_weights('best_weights_9.hdf5')\n",
    "\n",
    "    # we chose to train the top 2 conv blocks, i.e. we will freeze\n",
    "    # the first 8 layers and unfreeze the rest:\n",
    "    for i, layer in enumerate(classifier.layers[:20]):\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compiling the Model\n",
    "    classifier.compile(optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9, decay=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "   \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "def create_model_D(activation='softmax', learning_rate=0.01):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(MaxPooling2D((2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation=activation))\n",
    "    \n",
    "    opt = Adam(lr=learning_rate) \n",
    "    \n",
    "    classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "def create_model_01(activation='softmax', learning_rate=0.01):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)))\n",
    "    classifier.add(MaxPooling2D((2, 2), strides = (2, 2), padding = 'valid'))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    #classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    classifier.add(BatchNormalization())\n",
    "\n",
    "    #classifier.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    classifier.add(Dense(38,activation=activation))\n",
    "    \n",
    "    opt = Adam(lr=learning_rate)  \n",
    "    \n",
    "    classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import keras\n",
    "\n",
    "def create_model_vgg16(activation='softmax', learning_rate=0.01):\n",
    "    base_model=VGG16(include_top=False,input_shape=(224,224,3))\n",
    "    base_model.trainable=False\n",
    "\n",
    "    classifier=keras.models.Sequential()\n",
    "    classifier.add(base_model)\n",
    "    classifier.add(Flatten())\n",
    "    classifier.add(Dense(38,activation=activation))\n",
    "\n",
    "    opt = Adam(lr=learning_rate)    \n",
    "    classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Alexnet  - Best Score:  0.9791666666666666 and Best Params:  {'learning_rate': 0.01, 'epochs': 3, 'batch_size': 64, 'activation': 'softmax'}\nVgg16  - Best Score:  0.10416666666666667 and Best Params:  {'learning_rate': 0.001, 'epochs': 10, 'batch_size': 32, 'activation': 'softmax'}\nModel_D  - Best Score:  0.041666666666666664 and Best Params:  {'learning_rate': 0.01, 'epochs': 5, 'batch_size': 64, 'activation': 'softmax'}\n"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score\n",
    "from keras import regularizers\n",
    "\n",
    "# checkpoint for Alexnet Model\n",
    "weightpath = \"best_weights_9.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model_ids = ['Alexnet','Vgg16','Model_D']\n",
    "fn_names = [create_model_alexnet, create_model_vgg16, create_model_D]\n",
    "\n",
    "X_train, y_train = next(training_set)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': [\"softmax\"],\n",
    "          'batch_size': [32, 48, 64], \n",
    "          'epochs': [3, 5, 10],\n",
    "          'learning_rate': [0.01, 0.001, 0.0001]}\n",
    "for fn_name, model_id in zip(fn_names, model_ids):\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasClassifier(build_fn = fn_name, verbose = 0)\n",
    "\n",
    "    # Create a randomize search cv object passing in the parameters to try\n",
    "    random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3), verbose=0)    \n",
    "\n",
    "    if model_id == 'Alexnet':\n",
    "        random_search_results = random_search.fit(X_train, y_train, callbacks=callbacks_list)\n",
    "    else:\n",
    "        random_search_results = random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print best score and parameters\n",
    "    print(model_id, \" - Best Score: \", random_search_results.best_score_, \"and Best Params: \", random_search_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  5.2min finished\nAlexnet  - Best Score:  0.9583333333333334 and Best Params:  {'activation': 'softmax', 'batch_size': 32, 'epochs': 3, 'learning_rate': 0.01}\nFitting 3 folds for each of 27 candidates, totalling 81 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "model_ids = ['Alexnet','Vgg16','Model_D']\n",
    "fn_names = [create_model_alexnet, create_model_vgg16, create_model_D]\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': [\"softmax\"],\n",
    "          'batch_size': [32, 48, 64], \n",
    "          'epochs': [3, 5, 10],\n",
    "          'learning_rate': [0.01, 0.001, 0.0001]}\n",
    "\n",
    "X_train, y_train = next(training_set)\n",
    "\n",
    "for fn_name, model_id in zip(fn_names, model_ids):\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasClassifier(build_fn = fn_name, verbose = 0)\n",
    "\n",
    "    # Create a grid search cv object passing in the parameters to try\n",
    "    grid_search = GridSearchCV(model, params, cv = KFold(3), verbose=1)\n",
    "\n",
    "    if model_id == 'Alexnet':\n",
    "        grid_search_results = grid_search.fit(X_train, y_train, callbacks=callbacks_list)\n",
    "    else:\n",
    "        grid_search_results = grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print best score and parameters\n",
    "    print(model_id, \" - Best Score: \", grid_search_results.best_score_, \"and Best Params: \", grid_search_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}